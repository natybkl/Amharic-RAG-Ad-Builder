{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9BDzLVkUFT4"
      },
      "source": [
        "# Sentencepiece python module\n",
        "\n",
        "\n",
        "This notebook describes comprehensive examples of sentencepiece Python module.\n",
        "Since Python module calls C++ API through SWIG,  this document is also useful for developing C++ client."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgXb6P2Yg6g"
      },
      "source": [
        "## Install and data preparation\n",
        "\n",
        "We use the small training data (TIKVAH.txt) in this example.\n",
        "([TIKVAH-ETHIOPIA](https://t.me/tikvahethiopia) is a Telegram channel having over 1.2M subscribers.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k5KbVgiYae-"
      },
      "source": [
        "## Basic  end-to-end example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9W6wGnVteW",
        "outputId": "887ee49f-b08c-4acb-a731-5f95a54d0131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '▁ኤምባሲ']\n",
            "[460, 133, 774, 1276]\n",
            "_በአዲስ_አበባ_የአሜሪካ_ኤምባሲ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 39064 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 690 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=15511632\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9513% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=293\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999513\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 39064 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=7025048\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 292830 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 39064\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 263026\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 263026 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=139074 obj=12.5602 num_tokens=521494 num_tokens/piece=3.74976\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=119909 obj=10.929 num_tokens=523433 num_tokens/piece=4.36525\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=89897 obj=10.9168 num_tokens=552548 num_tokens/piece=6.14646\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=89803 obj=10.8929 num_tokens=553131 num_tokens/piece=6.15938\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=67352 obj=10.9945 num_tokens=593626 num_tokens/piece=8.81378\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=67351 obj=10.9667 num_tokens=593707 num_tokens/piece=8.81512\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=50512 obj=11.1144 num_tokens=637753 num_tokens/piece=12.6258\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=50512 obj=11.0804 num_tokens=637822 num_tokens/piece=12.6271\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37884 obj=11.2718 num_tokens=682809 num_tokens/piece=18.0237\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37884 obj=11.2304 num_tokens=682810 num_tokens/piece=18.0237\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=28413 obj=11.4629 num_tokens=727238 num_tokens/piece=25.5953\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=28413 obj=11.4137 num_tokens=727240 num_tokens/piece=25.5953\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=21309 obj=11.6949 num_tokens=772264 num_tokens/piece=36.2412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=21309 obj=11.6368 num_tokens=772283 num_tokens/piece=36.2421\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15981 obj=11.9729 num_tokens=817771 num_tokens/piece=51.1715\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15981 obj=11.9045 num_tokens=817795 num_tokens/piece=51.173\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11985 obj=12.2978 num_tokens=863488 num_tokens/piece=72.0474\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11985 obj=12.2184 num_tokens=863520 num_tokens/piece=72.0501\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8988 obj=12.6591 num_tokens=911287 num_tokens/piece=101.389\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8988 obj=12.5693 num_tokens=911330 num_tokens/piece=101.394\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6741 obj=13.0656 num_tokens=957322 num_tokens/piece=142.015\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6741 obj=12.9647 num_tokens=957370 num_tokens/piece=142.022\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5055 obj=13.5235 num_tokens=1009486 num_tokens/piece=199.7\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5055 obj=13.4106 num_tokens=1009539 num_tokens/piece=199.711\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3791 obj=14.0254 num_tokens=1060267 num_tokens/piece=279.68\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3791 obj=13.9023 num_tokens=1060289 num_tokens/piece=279.686\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2843 obj=14.5518 num_tokens=1112357 num_tokens/piece=391.262\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2843 obj=14.4122 num_tokens=1113435 num_tokens/piece=391.641\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.0204 num_tokens=1156472 num_tokens/piece=525.669\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=14.8939 num_tokens=1156476 num_tokens/piece=525.671\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train sentencepiece model from `TIKVAH.txt` and makes `m.model` and `m.vocab`\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\n",
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\n",
        "print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "በአዲስ አበባ የአሜሪካ ኤምባሲ\n"
          ]
        }
      ],
      "source": [
        "print(sp.decode_ids([460, 133, 774, 1276]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "4vHnQbBOltZo",
        "outputId": "9bb1ecaf-2883-494c-e34b-5616efac126a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\n",
            "▁በአዲስ\n",
            "460\n",
            "0\n",
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n"
          ]
        }
      ],
      "source": [
        "# returns vocab size\n",
        "print(sp.get_piece_size())\n",
        "\n",
        "# id <=> piece conversion\n",
        "print(sp.id_to_piece(460))\n",
        "print(sp.piece_to_id('▁በአዲስ'))\n",
        "\n",
        "# returns 0 for unknown tokens (we can change the id for UNK)\n",
        "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
        "\n",
        "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
        "# <s> and </s> are defined as 'control' symbol.\n",
        "for id in range(3):\n",
        "  print(sp.id_to_piece(id), sp.is_control(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRv6EeC2Y2PE"
      },
      "source": [
        "## Loads model from byte stream\n",
        "\n",
        "Sentencepiece's model file is just a serialized [protocol buffer](https://developers.google.com/protocol-buffers/). We can instantiate sentencepiece processor from byte object with **load_from_serialized_proto** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0Bdi9SuxYAud",
        "outputId": "b1566541-288e-4aa3-9c75-e494d0ab276a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-23 14:50:27.361318: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-01-23 14:50:30.542459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 14:50:30.543243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 14:50:31.171702: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 14:50:32.652438: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-01-23 14:50:32.663901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-23 14:50:36.043446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '▁ኤምባሲ']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assumes that m.model is stored in non-Posix file system.\n",
        "serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load_from_serialized_proto(serialized_model_proto)\n",
        "\n",
        "print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imfPyYlVZmxz"
      },
      "source": [
        "## User defined and control symbols\n",
        "\n",
        "We can define special tokens (symbols) to tweak the DNN behavior through the tokens.   Typical examples are  [BERT](https://arxiv.org/abs/1810.04805)'s special symbols., e.g., [SEP] and [CLS].\n",
        "\n",
        "There are two types of special tokens:\n",
        "\n",
        "- **user defined symbols**: Always treated as one token in any context. These symbols can appear in the input sentence.\n",
        "- **control symbol**:  We only reserve ids for these tokens. Even if these tokens appear in the input text, they are not handled as one token. User needs to insert ids explicitly after encoding.\n",
        "\n",
        "For experimental purpose, user defined symbols are easier to use since user can change the behavior just by modifying the input text. However,  we want to use control symbols in the production setting in order to avoid users from tweaking the behavior by feeding these special symbols in their input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "dngckiPMcWbA",
        "outputId": "e52883b1-6452-4a90-8802-945c9ac9d5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '<sep>', '▁ኤምባሲ', '<cls>']\n",
            "3\n",
            "4\n",
            "3= <sep>\n",
            "4= <cls>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_user\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: <sep>\n",
            "  user_defined_symbols: <cls>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_user.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_user.vocab\n"
          ]
        }
      ],
      "source": [
        "# Example of user defined symbols\n",
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_user = spm.SentencePieceProcessor()\n",
        "sp_user.load('m_user.model')\n",
        "\n",
        "# ids are reserved in both mode.\n",
        "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# user defined symbols allow these symbol to apper in the text.\n",
        "print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\n",
        "print(sp_user.piece_to_id('<sep>'))  # 3\n",
        "print(sp_user.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
        "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "5awRJ0y1oYm-",
        "outputId": "a5fa1ef9-ee5f-4f7d-b6bd-b611979b7350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '<', 's', 'e', 'p', '>', 'ኤ', 'ም', 'ባ', 'ሲ', '<', 'c', 'l', 's', '>']\n",
            "3\n",
            "4\n",
            "3= \n",
            "4= \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_ctrl\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  control_symbols: <sep>\n",
            "  control_symbols: <cls>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_ctrl.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_ctrl.vocab\n"
          ]
        }
      ],
      "source": [
        "# Example of control symbols\n",
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_ctrl = spm.SentencePieceProcessor()\n",
        "sp_ctrl.load('m_ctrl.model')\n",
        "\n",
        "# control symbols just reserve ids.\n",
        "print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\n",
        "print(sp_ctrl.piece_to_id('<sep>'))  # 3\n",
        "print(sp_ctrl.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_ctrl.decode_ids([3]))  # decoded to empty\n",
        "print('4=', sp_ctrl.decode_ids([4]))  # decoded to empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ppZck91s0rq"
      },
      "source": [
        " BOS/EOS (&lt;s&gt;, &lt;/s&gt;) are defined as control symbols, but we can define them as user defined symbols."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "PQoZ8paVhcEL",
        "outputId": "c17d2ae2-17cd-4875-997c-fe66dba23dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', '<', 's', '>', '▁በአዲስ', '<', '/', 's', '>']\n",
            "['▁', '<s>', '▁በአዲስ', '</s>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_bos_as_user\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: <s>\n",
            "  user_defined_symbols: </s>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_bos_as_user.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_bos_as_user.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces('<s> በአዲስ</s>'))   # <s>,</s> are segmented. (default behavior)\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m_bos_as_user.model')\n",
        "print(sp.encode_as_pieces('<s> በአዲስ</s>'))   # <s>,</s> are handled as one token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ2GjO5Tmjk9"
      },
      "source": [
        "## Manipulating BOS/EOS/EOS/PAD symbols\n",
        "\n",
        "BOS, EOS, UNK, and PAD ids can be obtained with **bos_id()**, **eos_id()**,  **unk_id()**, and **pad_id()** methods. We can explicitly insert these ids as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "UtFQqK3tmp7G",
        "outputId": "03e82bec-be40-4574-ab62-c66cdc3f28a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bos= 1\n",
            "eos= 2\n",
            "unk= 0\n",
            "pad= -1\n",
            "[434, 111]\n",
            "[1, 434, 111, 2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print('bos=', sp.bos_id())\n",
        "print('eos=', sp.eos_id())\n",
        "print('unk=', sp.unk_id())\n",
        "print('pad=', sp.pad_id())  # disabled by default\n",
        "\n",
        "\n",
        "print(sp.encode_as_ids('በአዲስ አበባ'))\n",
        "\n",
        "# Prepend or append bos/eos ids.\n",
        "print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDXA3Q6kjCS"
      },
      "source": [
        "## Sampling and nbest segmentation for subword regularization\n",
        "\n",
        "When **--model_type=unigram** (default) is used,  we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "nSQp93qflZO3",
        "outputId": "d5b45b62-2789-4879-b80b-f441e8b594af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አ', 'በ', 'ባ']\n",
            "['▁በአዲስ', '▁አ', 'በ', 'ባ']\n",
            "['▁በአዲስ', '▁አበባ']\n",
            "['▁በ', 'አዲስ', '▁አበባ']\n",
            "['▁በአዲስ', '▁አበባ']\n",
            "['▁በአዲስ', '▁አበባ']\n",
            "['▁በ', 'አ', 'ዲ', 'ስ', '▁አ', 'በ', 'ባ']\n",
            "['▁', 'በ', 'አዲስ', '▁አ', 'በ', 'ባ']\n",
            "['▁በአ', 'ዲ', 'ስ', '▁አ', 'በ', 'ባ']\n",
            "['▁በአ', 'ዲ', 'ስ', '▁አበባ']\n",
            "[434, 111]\n",
            "[434, 111]\n",
            "[8, 1066, 111]\n",
            "[8, 1066, 111]\n",
            "[3, 34, 1066, 111]\n",
            "[8, 130, 110, 13, 16, 34, 41]\n",
            "[434, 3, 130, 34, 41]\n",
            "[434, 111]\n",
            "[3, 34, 1066, 3, 130, 34, 41]\n",
            "[8, 1066, 111]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# Can obtain different segmentations per request.\n",
        "# There are two hyperparamenters for sampling (nbest_size and inverse temperature). see the paper [kudo18] for detail.\n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_pieces('በአዲስ አበባ', -1, 0.1))\n",
        "\n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_ids('በአዲስ አበባ', -1, 0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "9V1snUZdlb_v",
        "outputId": "bae2ca99-aca3-4013-c240-53b09a0bb684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['▁በአዲስ', '▁አበባ'], ['▁በ', 'አዲስ', '▁አበባ'], ['▁በአዲስ', '▁አ', 'በ', 'ባ'], ['▁', 'በ', 'አዲስ', '▁አበባ'], ['▁በአ', 'ዲ', 'ስ', '▁አበባ'], ['▁በአዲስ', '▁', 'አ', 'በ', 'ባ'], ['▁በ', 'አ', 'ዲ', 'ስ', '▁አበባ'], ['▁በ', 'አዲስ', '▁አ', 'በ', 'ባ'], ['▁', 'በ', 'አ', 'ዲ', 'ስ', '▁አበባ'], ['▁', 'በ', 'አዲስ', '▁አ', 'በ', 'ባ']]\n",
            "[[434, 111], [8, 1066, 111], [434, 16, 34, 41], [3, 34, 1066, 111], [188, 110, 13, 111], [434, 3, 130, 34, 41], [8, 130, 110, 13, 111], [8, 1066, 16, 34, 41], [3, 34, 130, 110, 13, 111], [3, 34, 1066, 16, 34, 41]]\n"
          ]
        }
      ],
      "source": [
        "# get 10 best\n",
        "print(sp.nbest_encode_as_pieces('በአዲስ አበባ', 10))\n",
        "print(sp.nbest_encode_as_ids('በአዲስ አበባ', 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6cxuVNcDKh"
      },
      "source": [
        "## BPE (Byte pair encoding) model\n",
        "\n",
        "Sentencepiece supports BPE (byte-pair-encoding) for subword segmentation with **--model_type=bpe** flag.   We do not find empirical differences in translation quality between BPE and unigram model, but unigram model can perform sampling and n-best segmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MNQxuX4Mc0KY",
        "outputId": "a6ed3e99-46c3-4c5e-dc8e-80394e17e363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** BPE ***\n",
            "['▁በአዲስ', 'አ', 'በ', 'ባ', 'የ', 'አ', 'ሜሪካ', 'ኤ', 'ምባ', 'ሲ']\n",
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_bpe\n",
            "  model_type: BPE\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=105325 min_freq=791\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13879 size=20 all=20266 active=3280 piece=▁ሰ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7808 size=40 all=21877 active=4891 piece=▁ክ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6491 size=60 all=23393 active=6407 piece=ጵያ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4963 size=80 all=24730 active=7744 piece=▁ኮ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4087 size=100 all=25905 active=8919 piece=▁አካ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4072 min_freq=538\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3567 size=120 all=26970 active=2336 piece=▁ዶ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3272 size=140 all=28072 active=3438 piece=▁ሳ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2959 size=160 all=29207 active=4573 piece=▁አስተ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2689 size=180 all=30138 active=5504 piece=ሊስ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2412 size=200 all=31078 active=6444 piece=▁ቀን\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2410 min_freq=413\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2294 size=220 all=32106 active=2564 piece=▁V\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2122 size=240 all=33074 active=3532 piece=▁ተና\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1993 size=260 all=34129 active=4587 piece=▁አለ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1854 size=280 all=34963 active=5421 piece=ተው\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1735 size=300 all=36013 active=6471 piece=▁ገልፀ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1733 min_freq=327\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1655 size=320 all=36636 active=2415 piece=ሰባ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1564 size=340 all=37391 active=3170 piece=ስቲ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1495 size=360 all=38164 active=3943 piece=▁ኣም\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1415 size=380 all=38820 active=4599 piece=▁በዚህ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1341 size=400 all=39698 active=5477 piece=ላለ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1335 min_freq=282\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1279 size=420 all=40472 active=2661 piece=▁ዋና\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1237 size=440 all=41149 active=3338 piece=ማር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1199 size=460 all=41991 active=4180 piece=▁ሽ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1131 size=480 all=42726 active=4915 piece=▁ሆነ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1080 size=500 all=43463 active=5652 piece=▁ቃ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1077 min_freq=247\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1038 size=520 all=44186 active=2861 piece=▁ሶ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1005 size=540 all=44802 active=3477 piece=▁ወይ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=959 size=560 all=45284 active=3959 piece=መር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=932 size=580 all=46043 active=4718 piece=ከያ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=910 size=600 all=46579 active=5254 piece=▁ሀገር\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=905 min_freq=219\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=880 size=620 all=47114 active=2854 piece=VAH\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=859 size=640 all=47743 active=3483 piece=ሜሪካ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=838 size=660 all=48462 active=4202 piece=▁ይገባ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=809 size=680 all=49075 active=4815 piece=ላንት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=785 size=700 all=49969 active=5709 piece=▁ፈተና\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=784 min_freq=193\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=765 size=720 all=50375 active=2887 piece=በው\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=744 size=740 all=51211 active=3723 piece=▁ግንባ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=725 size=760 all=51709 active=4221 piece=ጣት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=706 size=780 all=52325 active=4837 piece=▁የሚያስ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=688 size=800 all=52989 active=5501 piece=ስፒ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=688 min_freq=172\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=672 size=820 all=53709 active=3359 piece=ገለፁ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=659 size=840 all=54359 active=4009 piece=ብሄ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=647 size=860 all=54842 active=4492 piece=ህል\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=634 size=880 all=55517 active=5167 piece=ቅረ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=623 size=900 all=56137 active=5787 piece=ጠት\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=622 min_freq=156\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=614 size=920 all=56484 active=3140 piece=ስፋ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=600 size=940 all=57146 active=3802 piece=ኳን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=586 size=960 all=57616 active=4272 piece=▁ዝግጅት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=576 size=980 all=58091 active=4747 piece=▁ከሰ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=561 size=1000 all=58865 active=5521 piece=▁አፈ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=561 min_freq=142\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=553 size=1020 all=59506 active=3560 piece=▁መጠ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=540 size=1040 all=59904 active=3958 piece=▁ፕሬዚ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=529 size=1060 all=60242 active=4296 piece=ራቸው\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=519 size=1080 all=60893 active=4947 piece=▁ቤተሰ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=510 size=1100 all=61396 active=5450 piece=ግብ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=510 min_freq=131\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=503 size=1120 all=61832 active=3450 piece=▁የሃ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=491 size=1140 all=62322 active=3940 piece=▁በከተማ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=480 size=1160 all=62924 active=4542 piece=ማን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=470 size=1180 all=63555 active=5173 piece=▁በዚህም\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=463 size=1200 all=63955 active=5573 piece=▁መም\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=462 min_freq=122\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=455 size=1220 all=64555 active=3783 piece=▁ዳይሬክተር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=446 size=1240 all=65184 active=4412 piece=▁ምላሽ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=439 size=1260 all=65672 active=4900 piece=▁የፈ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=433 size=1280 all=66087 active=5315 piece=ሄድ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=426 size=1300 all=66644 active=5872 piece=▁ከተሞች\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=426 min_freq=113\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=419 size=1320 all=67173 active=3857 piece=ሬሽን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=412 size=1340 all=67622 active=4306 piece=ሜን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=405 size=1360 all=68044 active=4728 piece=▁ደስ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=399 size=1380 all=68609 active=5293 piece=ያዝ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=391 size=1400 all=69089 active=5773 piece=አዴግ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=391 min_freq=105\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=384 size=1420 all=69647 active=4005 piece=ረር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=378 size=1440 all=70193 active=4551 piece=▁የሶ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=373 size=1460 all=70656 active=5014 piece=ኛውም\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=368 size=1480 all=71142 active=5500 piece=▁በበኩ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=363 size=1500 all=71624 active=5982 piece=▁እየተካሄደ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=362 min_freq=98\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=357 size=1520 all=72040 active=3991 piece=▁እንዳል\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=351 size=1540 all=72411 active=4362 piece=ገነ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=345 size=1560 all=72966 active=4917 piece=ሳሳይ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=341 size=1580 all=73472 active=5423 piece=ንደ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=337 size=1600 all=73995 active=5946 piece=ለጠ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=337 min_freq=92\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=332 size=1620 all=74270 active=3926 piece=▁ኣመታት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=328 size=1640 all=74809 active=4465 piece=▁ጊ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=322 size=1660 all=75210 active=4866 piece=▁ወይዘ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=318 size=1680 all=75502 active=5158 piece=ብን\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_bpe.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_bpe.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "print('*** BPE ***')\n",
        "print(sp_bpe.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))\n",
        "print(sp_bpe.nbest_encode_as_pieces('በአዲስ አበባ', 5))  # returns an empty list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "EZrj1zCkvK8v",
        "outputId": "8d5421d9-9b89-440d-c91a-f03c081947d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Unigram ***\n",
            "['▁በአዲስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ']\n",
            "[['▁በአዲስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ'], ['▁በ', 'አዲስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ'], ['▁', 'በ', 'አዲስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ'], ['▁በአ', 'ዲ', 'ስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ'], ['▁በ', 'አ', 'ዲ', 'ስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_unigram\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_unigram.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_unigram.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')\n",
        "sp_unigram = spm.SentencePieceProcessor()\n",
        "sp_unigram.load('m_unigram.model')\n",
        "\n",
        "print('*** Unigram ***')\n",
        "print(sp_unigram.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))\n",
        "print(sp_unigram.nbest_encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ', 5))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
