{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove:\n",
    "* Null values, new_line(\"\\n\"), hashtags(\"#*\"), emojis, other characters\n",
    "### Replace:\n",
    "* ['·àê', '·àë', '·àí', '·àì', '·àî', '·àñ'] with ['·àÄ', '·àÅ', '·àÇ', '·àÉ', '·àÑ', '·àÖ', '·àÜ']\n",
    "* ['·äÄ', '·äÅ', '·äÇ', '·äÉ', '·äÑ', '·äÖ', '·äÜ'] with ['·àÄ', '·àÅ', '·àÇ', '·àÉ', '·àÑ', '·àÖ', '·àÜ']\n",
    "* ['·à†', '·à°', '·à¢', '·à£', '·à§', '·à¶', '·à¶', '·àß'] with ['·à∞, '·à±', '·à≤', '·à≥', '·à¥', '·àµ', '·à∂', '·à∑']\n",
    "* ['·ãê', '·ãë', '·ãí', '·ãì', '·ãî', '·ãï', '·ãñ'] with ['·ä†', '·ä°', '·ä¢', '·ä£', '·ä§', '·ä•', '·ä¶']\n",
    "* ['·å∏', '·åπ', '·å∫', '·åª', '·åº', '·åΩ', '·åæ'] with ['·çÄ', '·çÅ', '·çÇ', '·çÉ', '·çÑ', '·çÖ', '·çÜ']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, re, zipfile, csv\n",
    "import pandas as pd\n",
    "# sys.path.append(os.path.abspath(os.path.join('../scripts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util():\n",
    "    def __init__(self) -> None:\n",
    "        self.emoji_pattern = re.compile(\"[\"\n",
    "                                        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                                        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols and Pictographs\n",
    "                                        u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                                        u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "                                        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                                        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                                        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                                        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                                        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                                        u\"\\u2600-\\u26FF\"  # Miscellaneous Symbols\n",
    "                                        u\"\\u2700-\\u27BF\"  # Dingbats\n",
    "                                        u\"\\u2B50\"  # Star\n",
    "                                        u\"\\U0001F1E6-\\U0001F1FF\"  # Country Flags\n",
    "                                        \"]+\", flags=re.UNICODE)\n",
    "        self.symbols = re.compile(\"[\"\n",
    "                                  \"\\\"\"\n",
    "                                  \"\\‚Äú\"\n",
    "                                  \"\\\"\"\n",
    "                                  \"\\'\"\n",
    "                                  \"\\-\"\n",
    "                                  \"\\*\"\n",
    "                                  \"\\‚Ä¢\"\n",
    "                                  \"\\‚Ñπ\"\n",
    "                                  \"\\Ôªø\"\n",
    "                                  \"\\_\"\n",
    "                                  \"]+\")\n",
    "        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        self.mention_pattern = r'@(\\w+)'\n",
    "\n",
    "    def read_file(self, file_path: str) -> dict:\n",
    "        # Open the file for reading\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Load the JSON data from the file\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "\n",
    "    def write_file(self, file_path: str, data: dict) -> None:\n",
    "        # Open the file for writing\n",
    "        with open(file_path, 'w') as file:\n",
    "            # Dump the JSON data to the file\n",
    "            json.dump(data, file, indent=2)\n",
    "\n",
    "    def parse_text(self, text: any) -> str:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, list):\n",
    "            contents = []\n",
    "            for item in text:\n",
    "                if isinstance(item, str):\n",
    "                    contents.append(item)\n",
    "                elif isinstance(item, dict):\n",
    "                    contents.append(item['text'])\n",
    "            return \"\".join(contents)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def parse_messages(self, messages: list) -> dict:\n",
    "        parsed_messages = {\n",
    "            'id': [],\n",
    "            'text': [],\n",
    "            'date': []\n",
    "        }\n",
    "        for message in messages:\n",
    "            if message['type'] != 'message' or len(message['text']) == 0:\n",
    "                continue\n",
    "            parsed_messages['id'].append(message['id'])\n",
    "            message_content = self.parse_text(message['text'])\n",
    "            parsed_messages['text'].append(message_content)\n",
    "            parsed_messages['date'].append(message['date'])\n",
    "        return parsed_messages\n",
    "\n",
    "    def extract_hashtags(self, text: str) -> list:\n",
    "        return [word for word in text.split() if word.startswith('#')]\n",
    "\n",
    "    def extract_emojis(self, text):\n",
    "        return ''.join(self.emoji_pattern.findall(text))\n",
    "\n",
    "    def remove_emojis(self, text):\n",
    "        return self.emoji_pattern.sub('', text)\n",
    "\n",
    "    def extract_symbols(self, text):\n",
    "        return ''.join(self.symbols.findall(text))\n",
    "\n",
    "    def remove_symbols(self, text):\n",
    "        return self.symbols.sub(' ', text)\n",
    "\n",
    "    def extract_urls(self, text):\n",
    "        return re.findall(self.url_pattern, text)\n",
    "\n",
    "    def extract_mentions(self, text):\n",
    "        return re.findall(self.mention_pattern, text)\n",
    "    \n",
    "    def extract_fields(self, message):\n",
    "        \"\"\"\n",
    "        Extracts relevant fields from the message.\n",
    "        Returns a tuple containing (channel_id, text, date, labels).\n",
    "        \"\"\"\n",
    "        text = ' '.join(item['text'] for item in message['text_entities'] if item['type'] in 'plain')\n",
    "        date = message['date']\n",
    "        labels = \"LABEL\"  # Replace 'your_label' with the actual label(s) relevant to your use case\n",
    "        return text, date, labels\n",
    "\n",
    "    def process_json_file(self, json_file, csv_writer):\n",
    "        \"\"\"\n",
    "        Processes a JSON file, extracts relevant fields, and writes to CSV.\n",
    "        \"\"\"\n",
    "        data = json.load(json_file)\n",
    "\n",
    "        channel_id = data['id']\n",
    "        for message in data['messages']:\n",
    "            text, date, labels = self.extract_fields(message)\n",
    "            csv_writer.writerow([channel_id, text, date, labels])\n",
    "\n",
    "    def process_zip(self, zip_file_path, output_csv_path):\n",
    "        \"\"\"\n",
    "        Processes a zip file, extracts data from JSON files, and writes to a CSV file.\n",
    "        \"\"\"\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "            with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "                csv_writer = csv.writer(csv_file)\n",
    "                csv_writer.writerow(['id', 'text', 'date', 'label'])\n",
    "\n",
    "                for file_info in zip_file.infolist():\n",
    "                    with zip_file.open(file_info.filename) as json_file:\n",
    "                        print(json_file)\n",
    "                        self.process_json_file(json_file, csv_writer)\n",
    "                        \n",
    "    def file_reader(self, path: str, ) -> str:\n",
    "        fname = os.path.join(path)\n",
    "        with open(fname, 'r') as f:\n",
    "            system_message = f.read()\n",
    "        return system_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Util()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the zip data file\n",
    "\n",
    "We will create a csv file with the following files from all json files found in the zip file containing telegram channel infos in json format. The parsed file will have the following cols.\n",
    "\n",
    "\n",
    "- id: Telegram Channel ID\n",
    "- text: message content\n",
    "- date: message broadcast datetime.\n",
    "- label (s): one or more data labels relevant to your supervised training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zipfile.ZipExtFile name='4-3-3 FAST SPORT‚Ñ¢.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='90 ·ã∞·âÇ·âÉ ·àµ·çñ·à≠·âµ‚Ñ¢.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='DID U KNOWÔ∏è‚ÅâÔ∏è.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='DREAM APP‚Ñ¢.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='DREAM SPORT ‚Ñ¢.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='ETHIO ARSENAL.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='ETHIO-MEREJA¬Æ.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='Ethio University News¬Æ.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='History üìö.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='Manchester United Fans‚Ñ¢.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='QUBEE ACADEMY.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='Sheger PressÔ∏èÔ∏è.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='THE GOAT LM‚ôæ üêê.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='TIKVAH.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='Wasu Mohammed(·ãã·à± ·àò·àÄ·àò·ãµ).json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='YeneTube.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='·â•·àµ·à´·âµ ·àµ·çñ·à≠·âµ.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='·ä†·àµ·ã∞·äì·âÇ ·ä•·ãç·äê·â≥·ãé·âΩ üåç.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='·ä†·ã≤·àµ ·äê·åà·à≠ ·àò·à®·åÉ.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='·ä¢·âµ·ãÆ ·àò·à®·åÉ - NEWS.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='·ä¢·âµ·ãÆ ·à™·ã´·àç ·àõ·ãµ·à™·ãµ.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='·ã≥·åâ ·àµ·çñ·à≠·âµ DAGU SPORT.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='üíïüéÑ·çç·âÖ·à≠·äï ·â†·âÉ·àã·âµüéÑüíï.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='üî∂Wonderü§î.json' mode='r' compress_type=deflate>\n",
      "<zipfile.ZipExtFile name='üòçBest Profile Picturesüòç.json' mode='r' compress_type=deflate>\n",
      "Parsing completed. Output saved to ../data/parsed/parsed.csv\n"
     ]
    }
   ],
   "source": [
    "# zip_file_path = os.path.join(\"../data/raw/raw.zip\") \n",
    "# output_csv_path = os.path.join(\"../data/parsed/parsed.csv\") \n",
    "zip_file_path = \"../data/raw/raw.zip\"\n",
    "output_csv_path = \"../data/parsed/parsed.csv\"\n",
    "\n",
    "util.process_zip(zip_file_path, output_csv_path)\n",
    "print(\"Parsing completed. Output saved to\", output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the parsed data\n",
    "\n",
    "We will cleaning the text column of the parsed file. We will be cleaning the following things:\n",
    "\n",
    "1. **Clean Null Values:** Remove null or empty values in the 'text' column.\n",
    "   \n",
    "2. **Clean New Lines:** Remove extra line breaks or new lines from the 'text' column.\n",
    "\n",
    "3. **Remove Hashtags:** Remove hashtags from the 'text' column.\n",
    "\n",
    "4. **Remove Emojis:** Remove emojis from the 'text' column.\n",
    "\n",
    "5. **Remove Symbols:** Remove special symbols from the 'text' column.\n",
    "\n",
    "6. **Remove Links:** Remove hyperlinks or URLs from the 'text' column.\n",
    "\n",
    "7. **Remove Mentions:** Remove mentions or references (e.g., @username) from the 'text' column.\n",
    "\n",
    "8. **Remove Extra Spaces:** Remove extra spaces, multiple spaces, or leading/trailing spaces from the 'text' column.\n",
    "\n",
    "9. **Remove Non-Amharic Characters:** Remove characters that are not part of the Amharic script from the 'text' column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths\n",
    "parsed_csv_path = \"../data/parsed/parsed.csv\"\n",
    "output_cleaned_csv_path = \"../data/parsed/cleaned_parsed.csv\"\n",
    "\n",
    "# Read the parsed CSV into a DataFrame\n",
    "df = pd.read_csv(parsed_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply method to clean null or empty values in the 'text' column\n",
    "df['text'] = df['text'].apply(util.parse_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean New Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove newlines from the 'text' column\n",
    "df['text'] = df['text'].replace('\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hashtags from the 'text' column\n",
    "df['text'] = df['text'].str.replace(r'\\#\\w+', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply method to remove emojis from the 'text' column\n",
    "df['text'] = df['text'].apply(util.remove_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove Symbols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply method to remove special symbols from the 'text' column\n",
    "df['text'] = df['text'].apply(util.remove_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove Links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply method to remove hyperlinks or URLs from the 'text' column\n",
    "df['text'] = df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Remove Mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Remove Extra Spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_102058/4266213702.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df['text'] = df['text'].str.replace('\\s+', ' ', regex=True).str.strip()\n"
     ]
    }
   ],
   "source": [
    "# Apply method to remove extra spaces, multiple spaces, or leading/trailing spaces from the 'text' column\n",
    "df['text'] = df['text'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "df['text'] = df['text'].replace(r'!+', '!', regex=True)\n",
    "df['text'] = df['text'].replace(r'\\.+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Replace\n",
    "* ['·àê', '·àë', '·àí', '·àì', '·àî', '·àñ'] with ['·àÄ', '·àÅ', '·àÇ', '·àÉ', '·àÑ', '·àÖ', '·àÜ']\n",
    "* ['·äÄ', '·äÅ', '·äÇ', '·äÉ', '·äÑ', '·äÖ', '·äÜ'] with ['·àÄ', '·àÅ', '·àÇ', '·àÉ', '·àÑ', '·àÖ', '·àÜ']\n",
    "* ['·à†', '·à°', '·à¢', '·à£', '·à§', '·à¶', '·à¶', '·àß'] with ['·à∞, '·à±', '·à≤', '·à≥', '·à¥', '·àµ', '·à∂', '·à∑']\n",
    "* ['·ãê', '·ãë', '·ãí', '·ãì', '·ãî', '·ãï', '·ãñ'] with ['·ä†', '·ä°', '·ä¢', '·ä£', '·ä§', '·ä•', '·ä¶']\n",
    "* ['·å∏', '·åπ', '·å∫', '·åª', '·åº', '·åΩ', '·åæ'] with ['·çÄ', '·çÅ', '·çÇ', '·çÉ', '·çÑ', '·çÖ', '·çÜ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = [\n",
    "  [['·àê', '·àë', '·àí', '·àì', '·àî', '·àñ'], ['·àÄ', '·àÅ', '·àÇ', '·àÉ', '·àÑ', '·àÖ', '·àÜ']],\n",
    "  [['·äÄ', '·äÅ', '·äÇ', '·äÉ', '·äÑ', '·äÖ', '·äÜ'], ['·àÄ', '·àÅ', '·àÇ', '·àÉ', '·àÑ', '·àÖ', '·àÜ']],\n",
    "  [['·à†', '·à°', '·à¢', '·à£', '·à§', '·à¶', '·à¶', '·àß'], ['·à∞', '·à±', '·à≤', '·à≥', '·à¥', '·àµ', '·à∂', '·à∑']],\n",
    "  [['·ãê', '·ãë', '·ãí', '·ãì', '·ãî', '·ãï', '·ãñ'], ['·ä†', '·ä°', '·ä¢', '·ä£', '·ä§', '·ä•', '·ä¶']],\n",
    "  [['·å∏', '·åπ', '·å∫', '·åª', '·åº', '·åΩ', '·åæ'], ['·çÄ', '·çÅ', '·çÇ', '·çÉ', '·çÑ', '·çÖ', '·çÜ']]\n",
    "]\n",
    "\n",
    "for letter in letters:\n",
    "  for i in range(len(letter[0])):\n",
    "    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Remove Non Amharic Characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean English characters from the 'text' column\n",
    "df['text'] = df['text'].str.replace(r'[A-Za-z]+', '', regex=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
